---

title: "Homework #02 — Optimised Solution"
author: "Statistical Methods in Data Science II (2024‑2025)"
date: "`r format(Sys.time(), '%d %B %Y')`"
output:
html\_document:
toc: false
theme: united
pdf\_document:
keep\_tex: yes
toc: false
header-includes:

* \usepackage{geometry}
* \geometry{verbose,tmargin=4cm,bmargin=3cm,lmargin=2.5cm,rmargin=2.5cm}
* \usepackage{enumitem}

---

> **Global reproducibility**   A single `set.seed()` is used at the top; all subsequent pseudo‑random draws are therefore deterministic.

```{r setup, include = FALSE}
###############################################################################
##  Global options                                                            ##
###############################################################################
set.seed(20240601)          # one seed for the whole document

options(width = 80)
knitr::opts_chunk$set(
  echo       = TRUE,
  message    = FALSE,
  warning    = FALSE,
  cache      = FALSE,
  fig.align  = 'center',
  fig.width  = 5,
  fig.height = 4,
  out.lines  = 25
)

suppressPackageStartupMessages({
  library(ggplot2)
  library(dplyr)
  library(tidyr)
  library(expm)     # matrix powers / eigen‑decomp.
  library(purrr)
})
```

---

# 1  Acceptance–Rejection sampler (Normal via Cauchy)

## 1·a   Normalising constant and analytic *k*

Let the un‑normalised kernel be
$q(x)=\exp\bigl\{-x^{2}/2\bigr\},\qquad x\in\mathbb R.$
The normalising constant is the Gaussian integral
$Z=\int_{-\infty}^{\infty}q(x)\,dx=\sqrt{2\pi}.$
Thus the target density is the standard normal

$$
\tilde f(x)=\frac{1}{\sqrt{2\pi}}\exp\!\left(-\frac{x^{2}}{2}\right)=\mathcal N(0,1).
$$

We choose the standard Cauchy proposal
\$g(x)=\dfrac1{\pi(1+x^{2})}\$.  The envelope constant

$$
  k=\sup_{x\in\mathbb R}\frac{\tilde f(x)}{g(x)}
    =\frac{\pi}{\sqrt{2\pi}}\sup_{x}\frac{\exp(-x^{2}/2)}{1+x^{2}}
$$

attains its maximum at \$x=0\$ by symmetry and monotonicity, so
$k = \frac{\pi}{\sqrt{2\pi}}\approx 1.253314.$
The **theoretical acceptance probability** is therefore \$1/k\approx0.798\$.  This
analytic shortcut avoids numerical optimisation.

## 1·b   Vectorised A/R implementation

The sampler below draws proposals in *blocks* of size 5000, greatly reducing
R’s interpreter overhead; rejected proposals are stored in a pre‑allocated
buffer and trimmed afterwards.

```{r ar-sampler}
ar_normal <- function(n, block = 5e3, k = pi / sqrt(2 * pi)) {
  target   <- function(x) dnorm(x)                # Normal density
  proposal <- function(m) rcauchy(m)              # i.i.d. standard Cauchy draws
  g        <- function(x) dcauchy(x)              # Cauchy pdf

  out      <- numeric(n)                          # accepted samples
  rej_buf  <- numeric(5 * n)                      # rough upper bound
  acc      <- 0; rej <- 0                         # counters

  while (acc < n) {
    x  <- proposal(block)
    u  <- runif(block)
    fx <- target(x)
    gx <- g(x)
    keep <- u <= fx / (k * gx)

    nk          <- sum(keep)
    n_add       <- min(nk, n - acc)
    if (n_add > 0) out[(acc + 1):(acc + n_add)] <- x[keep][seq_len(n_add)]
    acc         <- acc + nk

    # store rejected proposals (only those generated *before* we hit n)
    rej_now     <- (!keep)
    r_count     <- sum(rej_now)
    if (r_count > 0) {
      if (rej + r_count > length(rej_buf)) rej_buf <- c(rej_buf, numeric(5 * n))
      rej_buf[(rej + 1):(rej + r_count)] <- x[rej_now]
      rej <- rej + r_count
    }
  }
  list(sample = out, rejected = rej_buf[seq_len(rej)], proposals = acc + rej)
}

## draw 10 000 accepted samples
ar_res <- ar_normal(1e4)
accept_rate <- length(ar_res$sample) / ar_res$proposals
accept_rate
```

```{r ar-theory-check, echo = FALSE}
cat(sprintf("Empirical acceptance %.3f (theory %.3f)\n",
            accept_rate, 1 / (pi / sqrt(2 * pi))))
```

The Monte‑Carlo estimate is within one standard error of the theoretical value.

## 1·c   Visual diagnostics

```{r ar-rejected-plot}
rejected_df   <- tibble(x = ar_res$rejected)

g_grid <- tibble(x = seq(-10, 10, length.out = 2001)) %>%
  mutate(Normal = dnorm(x),
         Cauchy = dcauchy(x) * (1 / k),   # scaled envelope
         Reject = (1 - dnorm(x) / (k * dcauchy(x))) * dcauchy(x)) %>%
  mutate(Reject = Reject / trapz::trapz(x, Reject))

# Overlay empirical rejection density and theory
rejected_df %>%
  ggplot(aes(x)) +
  geom_histogram(aes(y = ..density..), bins = 120, fill = "skyblue", alpha = .5) +
  geom_line(data = g_grid, aes(y = Reject), colour = "red", linewidth = 1) +
  coord_cartesian(xlim = c(-10, 10)) +
  labs(title   = "Distribution of rejected proposals",
       y       = "density",
       caption = "Red = theoretical rejection pdf") +
  theme_minimal()
```

---

# 2  Discrete three‑state Markov chain (“frog on rocks”)

Transition matrix

```{r transition-matrix}
P <- matrix(c(0,    0.5, 0.5,
              5/8,  1/4, 1/8,
              1/3,  2/3, 0),
            nrow = 3, byrow = TRUE,
            dimnames = list(1:3, 1:3))
P
```

The chain is **irreducible** and **aperiodic** (rows 1 and 2 have self‑loops),
so it converges to a unique stationary distribution $\boldsymbol\pi$.

## 2·a   Simulation helper

```{r sim-chain-fn, echo = FALSE}
sim_chain <- function(n, start, P) {
  states <- integer(n)
  states[1] <- start
  for (t in 2:n)
    states[t] <- sample.int(3L, 1L, prob = P[states[t - 1], ])
  states
}
```

## 2·b   One 1000‑step trajectory from state 1

```{r single-chain}
chain1 <- sim_chain(1000, 1, P)
(tab1   <- prop.table(table(chain1)))
```

```{r mc-trace-plot}
tibble(t = 1:1000, state = chain1) %>%
  ggplot(aes(t, state)) +
  geom_step() +
  scale_y_continuous(breaks = 1:3) +
  labs(title = "Trace plot of one trajectory") +
  theme_minimal()
```

## 2·c   Distribution of final state after 1000 steps (500 replicates)

```{r multi-start1}
end_states <- replicate(500, tail(sim_chain(1000, 1, P), 1))
(prop_end <- prop.table(table(end_states)))
```

## 2·d   Stationary distribution

```{r stationary}
pi_stat <- eigen(t(P))$vectors[, 1]
pi_stat <- Re(pi_stat / sum(pi_stat))
pi_stat
```

## 2·e   Convergence diagnostics

```{r diagnostics}
# helper: total variation distance
tv_dist <- function(p, q) 0.5 * sum(abs(p - q))

comp <- rbind(`empirical  chain1` = tab1,
              `500× final state` = prop_end,
              `stationary  pi`   = pi_stat)
round(comp, 3)

cat(sprintf("TV distance (chain1 vs pi) = %.3f\n", tv_dist(tab1, pi_stat)))
```

## 2·f   Different initial state

```{r start2}
end_states2 <- replicate(500, tail(sim_chain(1000, 2, P), 1))
prop.table(table(end_states2))
```

Empirical frequencies are again close to $\boldsymbol\pi$, confirming
*ergodicity*: the long‑run distribution is independent of the starting state.

---

```{r session-info, echo = FALSE}
cat("\n---\n")
cat(paste0("Last knit: ", format(Sys.time(), '%Y‑%m‑%d %H:%M:%S')))
```
